function net = backPropagation_nn(net, y, opt, epochNum)
numLayers = length(net.layers);

e = net.layers{numLayers}.a - y; % Total error

if opt.regression
    net.layers{numLayers}.d = e .* (net.layers{numLayers}.a .* (1 - net.layers{numLayers}.a));
    net.L = 1/2* sum(e(:) .^ 2) / size(e, 2); % Mean-squared loss for future checking
else
    net.layers{numLayers}.d = e;
    logy = log(net.layers{numLayers}.a);
    log1_y = log(1-net.layers{numLayers}.a);
    net.L = 0;
    for i = 1:size(y,1)
        net.L = net.L - y(i,:)*logy(i,:)' - (1-y(i,:))*log1_y(i,:)'; % Cross entropy
    end
    net.L = net.L/size(e, 2);
end

% Compute delta values for each layer
for l = (numLayers - 1) : -1 : 1
    if opt.gaussian
        grad = net.layers{l}.ga .* (net.layers{l}.a .* (1 - net.layers{l}.a));
    else
        grad = (net.layers{l}.a .* (1 - net.layers{l}.a));
    end
    if opt.dropconnect
        net.layers{l}.d = (net.layers{l + 1}.wdc' * net.layers{l + 1}.d) .* grad;
    else
        net.layers{l}.d = (net.layers{l + 1}.w' * net.layers{l + 1}.d) .* grad;
    end
end

% Perform gradient descent, no weights for max-pooling layer
if adaptive_alpha
    for l = 2 : numLayers
        net.layers{l}.b = net.layers{l}.b - opt.alpha * sum(net.layers{l}.d,2) / size(net.layers{l}.d,2);
        net.layers{l}.w = net.layers{l}.w - opt.alpha * net.layers{l}.d * net.layers{l - 1}.a' / size(net.layers{l}.d,2);
        alpha = opt.alpha_a/(opt.alpha_b + epochNum);
        if opt.dropconnect
            net.layers{l}.wdc = net.layers{l}.w .* net.layers{l-1}.dc;
        end
    end
else
    for l = 2 : numLayers
        net.layers{l}.b = net.layers{l}.b - opt.alpha * sum(net.layers{l}.d,2) / size(net.layers{l}.d,2);
        net.layers{l}.w = net.layers{l}.w - opt.alpha * net.layers{l}.d * net.layers{l - 1}.a' / size(net.layers{l}.d,2);
        if opt.dropconnect
            net.layers{l}.wdc = net.layers{l}.w .* net.layers{l-1}.dc;
        end
    end
end

end