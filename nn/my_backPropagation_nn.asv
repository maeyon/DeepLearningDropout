function net = my_backPropagation_nn(net, y, opt)
epochNum = net.iter;
numLayers = length(net.layers);
e = net.layers{numLayers}.a - y; % Total error

if opt.regression
    net.layers{numLayers}.d = e .* (net.layers{numLayers}.a .* (1 - net.layers{numLayers}.a));
    negL = 1/2*e(:).^ 2; % sample-wise negative log-likelihood
    net.L = mean(negL);% 1/2* sum(e(:) .^ 2) / size(e, 2); % Mean-squared loss for future checking
else
    net.layers{numLayers}.d = e;
    logy = log(net.layers{numLayers}.a);
    log1_y = log(1-net.layers{numLayers}.a);
    negL = 0; % sample-wise negative log-likelihood
    for i = 1:size(y,1)
        negL = negL - y(i,:).*logy(i,:) - (1-y(i,:)).*log1_y(i,:); % Cross entropy
    end
    net.L = mean(negL);%net.L/size(e, 2);
end

% Compute delta values for each layer
for l = (numLayers - 1) : -1 : 1
    if opt.gaussian
        grad = net.layers{l}.ga .* (net.layers{l}.a .* (1 - net.layers{l}.a));
    else
        grad = (net.layers{l}.a .* (1 - net.layers{l}.a));
    end
    if opt.dropconnect
        net.layers{l}.d = (net.layers{l + 1}.wdc' * net.layers{l + 1}.d) .* grad;
    else
        net.layers{l}.d = (net.layers{l + 1}.w' * net.layers{l + 1}.d) .* grad;
    end
end

% Perform gradient descent, no weights for max-pooling layer
if opt.adaptive_alpha
    alpha = opt.alpha_a/(opt.alpha_b + epochNum);
    for l = 2 : numLayers
        net.layers{l}.b = net.layers{l}.b - alpha * sum(net.layers{l}.d,2) / size(net.layers{l}.d,2);
        net.layers{l}.w = net.layers{l}.w - alpha * net.layers{l}.d * net.layers{l - 1}.a' / size(net.layers{l}.d,2);
        if opt.dropconnect
            net.layers{l}.wdc = net.layers{l}.w .* net.layers{l-1}.dc;
        end
    end
else
    for l = 2 : numLayers
        net.layers{l}.b = net.layers{l}.b - opt.alpha * sum(net.layers{l}.d,2) / size(net.layers{l}.d,2);
        net.layers{l}.w = net.layers{l}.w - opt.alpha * net.layers{l}.d * net.layers{l - 1}.a' / size(net.layers{l}.d,2);
        if opt.dropconnect
            net.layers{l}.wdc = net.layers{l}.w .* net.layers{l-1}.dc;
        end
    end
end

if isfield(opt, 'Bayesian_do')
    ido = sig(net.layers{1}.lambda);% opt.input_do_rate(epochNum);
    for l = 2:length(net.layers)-1
        hdo{l} = sig(net.layers{l}.lambda);%opt.hidden_do_rate(epochNum);
    end
    
    %%%%%%%%%%%%%%% update dropout rate in input layer 
    grad = 0;
    if strcmp(opt.Bayesian_do, 'UOR')
        temp = sum(net.layers{1}.do,1) -  ido;
        Nunits = size(net.layers{2}.w, 2);
        for l = 2:length(net.layers)-1
            temp = temp + sum(net.layers{l}.do,1) -  hdo{l};
            Nunits = Nunits + net.layers{l}.n;
        end
        logratio = log(opt.input_do_rate) - log(1-opt.input_do_rate);
        grad = mean(temp.*negL) - net.delta*(ido*(1-ido)*(log(1-ido)-log(ido)+logratio))*Nunits;
        if opt.adaptive_alpha
            net.layers{1}.lambda = net.layers{1}.lambda - alpha*grad;
        else
            net.layers{1}.lambda = net.layers{1}.lambda - opt.alpha*grad;
        end
        for l = 2:length(net.layers)-1
            net.layers{l}.lambda = net.layers{1}.lambda;
        end
    elseif strcmp(opt.Bayesian_do, 'UORH')
        grad = mean((sum(net.layers{1}.do,1) -  ido).*negL) - net.delta*(ido*(1-ido)*(log(1-ido)-log(ido)))*size(net.layers{2}.w, 2);
        temp = 0;
        for l = 2:length(net.layers)-1
            temp = temp + sum(net.layers{l}.do,1) -  hdo{l};
        end
        grad2 = mean(temp.*negL) - net.delta*( hdo{2}.*(1- hdo{2}).*(log(1- hdo{2})-log( hdo{2})));
        if opt.adaptive_alpha
            net.layers{1}.lambda = net.layers{1}.lambda - 0.001*alpha*grad;
            net.layers{2}.lambda = net.layers{2}.lambda - 0.001*alpha*grad2;
        else
            net.layers{1}.lambda = net.layers{1}.lambda - opt.alpha*grad;
            net.layers{2}.lambda = net.layers{2}.lambda - opt.alpha*grad2;
        end
        for l = 2:length(net.layers)-1
            net.layers{l}.lambda = net.layers{2}.lambda;
        end
    end
    
    %%%%%%%%%%%%%%% update dropout rate in hidden layers 
end

end